# 音高偵測技術的深入解析與比較

下面將針對四種常見的音高（基頻）偵測方法進行整理分析，包括它們的原理、計算流程、優缺點、適用場景，以及在 Rust 與 WebAssembly 環境下實作的可行性與建議。

## 1. **YIN 算法**

- **基本原理:** YIN 算法是基於自相關的時間域方法，但對傳統自相關進行了一系列修正以降低錯誤率​
    
    [hajim.rochester.edu](https://hajim.rochester.edu/ece/sites/zduan/teaching/ece477/projects/2015/Qiaozhan_Gao_report_ReportFinal.pdf#:~:text=YIN%20algorithm%20is%20designed%20for,perfectly%20suitable%20for%20the%20work)
    
    。具體而言，YIN 利用**差分函數**來衡量訊號與自身延遲後的差異，並透過**累積平均歸一化差分函數**（CMNDF）來強調真正的基頻位置，使得第一次出現的極小值對應於音高週期​
    
    [librosa.org](https://librosa.org/doc/main/generated/librosa.yin.html#:~:text=YIN%20is%20an%20autocorrelation%20based,converting%20into%20the%20corresponding%20frequency)
    
    。這些改進能有效避免一般自相關法常見的「八度誤差」等偵測錯誤​
    
    [hajim.rochester.edu](https://hajim.rochester.edu/ece/sites/zduan/teaching/ece477/projects/2015/Qiaozhan_Gao_report_ReportFinal.pdf#:~:text=YIN%20algorithm%20is%20designed%20for,perfectly%20suitable%20for%20the%20work)
    
    。
    
- **計算步驟:** YIN 演算法可分為以下步驟：
    
    1. **計算差分函數：**對每一個可能的延遲 $\tau$，計算差分函數 $d(\tau)=\sum_{i}[x_i - x_{i+\tau}]^2$。這相當於衡量訊號與其延遲版本之間的差異程度。
    2. **累積平均歸一化：**將上述差分函數進行歸一化處理，得到 $d'(\tau)$。歸一化方式為將 $d(\tau)$ 除以 $\frac{1}{\tau}\sum_{j=1}^{\tau}d(j)$（對 $\tau=0$ 特別定義為1），以消除差分值隨延遲增大而自然增大的趨勢​
        
        [hajim.rochester.edu](https://hajim.rochester.edu/ece/sites/zduan/teaching/ece477/projects/2015/Qiaozhan_Gao_report_ReportFinal.pdf#:~:text=Step%203%3A%20Cumulative%20mean%20normalized,mean%20normalized%20difference%20function%E2%80%99%E2%80%99%3A%20d2)
        
        。這會使真正的基頻週期處出現明顯的谷值。
    3. **設定阈值尋找谷值：**在歸一化後的差分函數 $d'(\tau)$ 中尋找首個低於預設阈值的局部最小值​
        
        [librosa.org](https://librosa.org/doc/main/generated/librosa.yin.html#:~:text=YIN%20is%20an%20autocorrelation%20based,converting%20into%20the%20corresponding%20frequency)
        
        。這個 $\tau$ 將被視為訊號的**週期估計值**，對應的頻率即初步的音高估計。
    4. **拋物線內插：**為了提高頻率估計的精度，對找到的極小值位置進行拋物線內插，細緻地計算非整數延遲下的差分函數谷底位置​
        
        [librosa.org](https://librosa.org/doc/main/generated/librosa.yin.html#:~:text=YIN%20is%20an%20autocorrelation%20based,converting%20into%20the%20corresponding%20frequency)
        
        。這能補償取樣限制，提高音高中頻率估計的精度。
    5. **結果輸出與濾除：**將內插後得到的週期取倒數轉換為頻率。如果差分谷值過高（表示訊號不夠週期性或背景雜訊過大），則可能判定為無音高訊號（如語音中的無聲段）。
- **優點:** YIN 算法對單音訊號的基頻估計具有高準確度和魯棒性。由於引入了差分函數歸一化和阈值檢測，YIN 能夠有效減少傳統自相關方法常見的八度錯誤，對噪聲的抵抗力也更佳​
    
    [hajim.rochester.edu](https://hajim.rochester.edu/ece/sites/zduan/teaching/ece477/projects/2015/Qiaozhan_Gao_report_ReportFinal.pdf#:~:text=YIN%20algorithm%20is%20designed%20for,perfectly%20suitable%20for%20the%20work)
    
    。實驗顯示，YIN 在同類演算法中運算速度和精確度表現突出，被認為是最快的基頻偵測法之一（平均處理一幀僅約 38 ms，速度大約是另一種頻域方法 SWIPE 的 9 倍）​
    
    [mdpi.com](https://www.mdpi.com/2076-3417/13/14/8191#:~:text=YIN%2C%20considered%20one%20of%20the,performances%20are%20too%2C%20with%20only)
    
    。此外，YIN 對週期波形的細微變化較敏感，可追蹤快速變化的音高曲線。
    
- **缺點:** YIN 雖然降低了許多偵測錯誤，但仍然主要適用於**單聲部/單音訊號**。當輸入包含多個同時發聲的音高（複音音訊）時，YIN 無法區分多個基頻，只能輸出一個頻率，基本實現不了複音情況的正確偵測​
    
    [en.wikipedia.org](https://en.wikipedia.org/wiki/Pitch_detection_algorithm#:~:text=can%20give%20quite%20accurate%20results,63)
    
    。另外，為了偵測較低的頻率，YIN 需要較長的分析窗口（至少包含兩個週期的訊號）​
    
    [en.wikipedia.org](https://en.wikipedia.org/wiki/Pitch_detection_algorithm#:~:text=Autocorrelation%20methods%20need%20at%20least,not%20necessarily%20have%20the%20same)
    
    ；例如要偵測 40 Hz 的基頻需至少約50毫秒的訊號長度，這在即時系統中會產生一定的延遲。計算方面，YIN 的差分函數計算本質上仍是 $O(N\tau_{\max})$（對每個候選延遲計算差異），當分析窗口較大時計算量不容忽視，但可透過演算法優化來降低平均運算量。
    
- **適用場景:** YIN 非常適合**單音的音高追蹤**，例如人聲、獨奏樂器的基頻偵測、卡拉OK即時音高反饋、樂器調音（單音階）等應用。在這些情況下，它相較簡單方法提供更高的準確率和穩定性，甚至在訊號稍有雜訊或泛音較強時仍能找出正確的基頻。在需要高精度且允許中等計算成本的即時系統中，YIN 是常用且可靠的選擇。
    
- **Rust 及 WebAssembly 的實作考量:** YIN 演算法相對簡潔，可使用 Rust 進行高效實作並編譯為 WebAssembly 在瀏覽器中運行。Rust 的強類型和記憶體安全特性有助於避免指標錯誤，而其性能接近底層語言，非常適合執行音頻處理中的大量數值計算。實作時可以將核心差分計算部份用向量化方式優化，或利用 FFT 快速卷積計算自相關來加速差分序列的生成。值得注意的是，在瀏覽器環境應確保以 **Release 模式**編譯 WebAssembly（優化開啟），避免 Debug 模式下運算緩慢導致的高CPU佔用​
    
    [bojandjurdjevic.com](https://bojandjurdjevic.com/2018/WASM-vs-JS-Realtime-pitch-detection/#:~:text=my%20microphone%2C%20and%20to%20output,35%25%20in%20Chromium%20on)
    
    。目前社群已有現成的 Rust 套件如 **pitch_detection** 提供了 YINDetector 等實作，可直接使用​
    
    [docs.rs](https://docs.rs/pitch-detection#:~:text=pitch_detection%20implements%20several%20algorithms%20for,usable%20in%20a%20WASM%20environment)
    
    ​
    
    [docs.rs](https://docs.rs/pitch-detection#:~:text=,YINDetector)
    
    。該套件特別設計可在 WASM 環境中使用，開發者也可以參考其實現。在 Rust/WASM 中使用 YIN 進行即時音高追蹤時，可搭配 Web Audio API 取得音訊緩衝並傳給 WASM 模組處理。實務經驗表明，Rust+WASM 的音高偵測相較純 JavaScript 可大幅提升性能（在 1024 點分析窗下，WASM 實現可比純 JS 加速約 8 倍以上）​
    
    [bojandjurdjevic.com](https://bojandjurdjevic.com/2018/WASM-vs-JS-Realtime-pitch-detection/#:~:text=To%20find%20it%20out%2C%20I,around)
    
    。總體而言，在 Rust 中實作 YIN 並發佈為 WASM，是**可行且高效**的方案。
    

## 2. **自相關法（Autocorrelation）**

- **公式與計算方式:** 自相關法是最基本的時域音高偵測方法之一。其核心在於計算訊號與自身延遲後的相似程度，即**自相關函數**。對於離散訊號 $x[n]$，自相關函數可定義為： Rx(τ)=∑n=0N−τ−1x[n]  x[n+τ]R_x(\tau) = \sum_{n=0}^{N-\tau-1} x[n]\;x[n+\tau]Rx​(τ)=∑n=0N−τ−1​x[n]x[n+τ] 其中 $\tau$ 是延遲樣本數，$N$ 是分析窗長度。直觀上，$R_x(\tau)$ 在 $\tau$ 等於基頻週期對應的延遲時會出現**峰值**，因為訊號與其一個週期後的形狀最相似。在實作中，我們通常會忽略 $\tau=0$ 的自相關（總能量）峰值，尋找 $\tau>0$ 的第一個主要峰值作為**音高週期**的估計。自相關計算可以直接在時域完成，也能透過快速傅立葉變換計算：由於時域的自相關相當於頻域的能量譜反Transform，因此可以對訊號做 FFT，將頻譜與其複共軛相乘，再做 IFFT 即可得到各延遲下的相關值。利用 FFT 技巧可將計算複雜度從原始的 $O(N^2)$ 降至 $O(N \log N)$，大幅提高計算效率​
    
    [en.wikipedia.org](https://en.wikipedia.org/wiki/Pitch_detection_algorithm#:~:text=Frequency%20domain%2C%20polyphonic%20detection%20is,suitably%20efficient%20for%20many%20purposes)
    
    。如果分析窗口長度較大或需要同時計算多個延遲的相關值，頻域卷積法會更有效率。
    
- **計算效率:** 純粹的時間域自相關計算在每幀需要對每個候選延遲做累加運算，對長訊號而言計算量龐大。然而，利用數學上的卷積關係，可以用FFT優化計算。例如對 1024 點訊號執行自相關，若直接計算可能耗時數十毫秒，而透過 FFT 只需幾毫秒甚至更少。FFT 的高效實現（如 **FFTW** 或 Rust 中的 `rustfft` 套件）在 C/C++ 或 Rust 中都能達到接近理論的性能，在 WebAssembly 環境下同樣可以受益​
    
    [bojandjurdjevic.com](https://bojandjurdjevic.com/2018/WASM-vs-JS-Realtime-pitch-detection/#:~:text=Note%20that%20this%20implementation%20is,seems%20like%20the%20best%20choice)
    
    。因此，在要求高實時性時，可考慮以 FFT 方法計算自相關，或只計算必要範圍內的延遲以降低運算量。此外，可以透過**預計算累積和**來快速計算不同 $\tau$ 下的平方和項，加速差值計算等變種（如平均差分函數 AMDF）。總體而言，自相關法在經過優化後能在實時應用中運行，其效率取決於窗口大小和目標頻率範圍。
    
- **優點:** 自相關法概念直觀且對**高度週期性的訊號**效果良好​
    
    [en.wikipedia.org](https://en.wikipedia.org/wiki/Pitch_detection_algorithm#:~:text=More%20sophisticated%20approaches%20compare%20segments,63)
    
    。它能直接在時域找出訊號的重複周期，對於諧波豐富但基頻能量不強的訊號（如方波、鋸齒波），自相關仍能透過週期性的出現來鎖定正確的基頻，而不受限於頻譜中基頻峰值的強弱。實作上也相對簡單，只需基本的加乘運算。由於其計算不涉及複數頻譜操作，在短幀長和合理的計算資源下，甚至可以在嵌入式裝置或手機上以固定小數點實現。自相關法還可以延伸應用，例如**平均絕對差分函數**(AMDF) 等都是類似原理的變體，可加強對雜訊的抵抗力。
    
- **缺點:** 標準自相關法存在幾個局限。首先，它容易出現**誤檢（尤其是八度誤差）**的問題：當訊號的第二諧波在頻譜中能量高於基頻時，自相關在基頻對應的延遲和其倍數（對應八度以下的頻率）都可能出現相關峰，導致誤將泛音周期當作基頻​****
    
    ****[en.wikipedia.org](https://en.wikipedia.org/wiki/Pitch_detection_algorithm#:~:text=difference%20function%20%29%2C%20ASMDF%20,citation%20needed)****
    
    ****。這在諧波結構複雜的聲音中很常見。其次，在**高雜訊**環境下，隨機噪聲會降低自相關峰與背景的對比，可能出現本不存在的偽峰值，使偵測不穩定​
    
    [en.wikipedia.org](https://en.wikipedia.org/wiki/Pitch_detection_algorithm#:~:text=can%20give%20quite%20accurate%20results,63)
    
    。再次，自相關法基本只適用於**單音訊號**：當同時存在多個不同頻率時，自相關函數是多個週期成分的疊加，往往無法直接區分出各別的基頻（需要進一步的訊號分離或多目標分析演算法）。最後，為了可靠地偵測最低頻率，需要至少兩個以上週期的訊號長度​
    
    [en.wikipedia.org](https://en.wikipedia.org/wiki/Pitch_detection_algorithm#:~:text=Autocorrelation%20methods%20need%20at%20least,not%20necessarily%20have%20the%20same)
    
    ；這對即時應用意味著**響應延遲**會增加，而且較長的分析窗也降低了對瞬時音高變化的時間解析度。
    
- **適用範圍:** 傳統自相關法適用於**單一穩態音調**的頻率估計，例如音叉聲、單音樂器長音、穩定的語音元音段等。在這類情況下，訊號週期清晰且雜訊較小，自相關能給出相當精確的結果。同時，由於自相關法實現簡單且不需要頻域轉換，它也常被用在計算資源有限的場景，例如早期的電子調音器、簡單的DSP晶片應用等。在需要低成本實作且允許一定誤差的場合（如玩具級或基礎實驗），自相關法提供了一種直觀可行的方案。不過在專業應用中，通常會對自相關法進行改進或結合其他方法，以提高在複雜情境下的可靠性。
    
- **Rust 及 WebAssembly 的實作建議:** 在 Rust 中實作自相關法非常直接，可使用迴圈計算逐點相關，或利用線性代數庫進行向量化計算。如果需要提升速度，建議採用 FFT 方法計算自相關：可以使用 Rust 的 `rustfft` 套件或將 FFTW 庫透過 FFI 綁定進行計算​
    
    [bojandjurdjevic.com](https://bojandjurdjevic.com/2018/WASM-vs-JS-Realtime-pitch-detection/#:~:text=Note%20that%20this%20implementation%20is,seems%20like%20the%20best%20choice)
    
    。由於 Rust 可以編譯為 WebAssembly，自相關計算核心可以放在 WASM 中執行，以加速瀏覽器端的音高分析。應注意在 WASM 中和主程式交換音訊資料的方式，可透過共享緩衝區或預先在 Rust 分配 memory，讓 JavaScript 將音訊樣本填入後直接計算，以減少拷貝開銷​
    
    [bojandjurdjevic.com](https://bojandjurdjevic.com/2018/WASM-vs-JS-Realtime-pitch-detection/#:~:text=The%20most%20difficult%20part%20was,this%20because%20it%E2%80%99s%20new%20technology)
    
    ​
    
    [bojandjurdjevic.com](https://bojandjurdjevic.com/2018/WASM-vs-JS-Realtime-pitch-detection/#:~:text=let%20ptr%20%3D%20Module._malloc%28bufferLength%20,BYTES_PER_ELEMENT)
    
    。另外，Rust 的 `pitch_detection` crate 中也提供了 AutocorrelationDetector，可以作為參考或直接使用​
    
    [docs.rs](https://docs.rs/pitch-detection#:~:text=A%20detector%20is%20an%20implementation,noise%20and%20polyphonic%20sounds%20varies)
    
    。在 Web 環境，若音訊幀長較大，也可考慮在 Web Worker 中執行 WASM 的自相關計算，避免阻塞主執行緒。在極端性能要求下，可以結合 WASM 的 SIMD 指令（現代瀏覽器已支持）來並行計算多個延遲的相關值。總的來說，自相關法在 Rust/WASM 上實作難度低，透過適當優化能勝任即時音高偵測需求。
    

## 3. **CREPE（深度學習音高估計模型）**

- **深度學習模型架構:** CREPE 原名為 “**Convolutional REpresentation for Pitch Estimation**”，是一種基於深度卷積神經網路的音高追蹤演算法​
    
    [github.com](https://github.com/marl/crepe#:~:text=CREPE%20is%20a%20monophonic%20pitch,such%20as%20pYIN%20and%20SWIPE)
    
    。值得注意的是，儘管縮寫上含有 R，但該模型並非傳統的「Recurrent（遞歸）」網路，而是**全卷積架構**。CREPE 的具體結構包含 **6 層一維卷積層**，直接對原始波形（時域訊號）進行特徵提取​
    
    [justinsalamon.com](https://www.justinsalamon.com/uploads/4/3/9/4/4394963/kim_crepe_icassp_2018.pdf#:~:text=Figure%201,com%2Fmarl%2Fcrepe)
    
    。輸入通常是長度 1024 個採樣點的音訊片段（約對應 16kHz 取樣率下64ms的訊號）​
    
    [justinsalamon.com](https://www.justinsalamon.com/uploads/4/3/9/4/4394963/kim_crepe_icassp_2018.pdf#:~:text=Figure%201,com%2Fmarl%2Fcrepe)
    
    。卷積堆疊後接上一個**全連接層**，輸出一個長度為 360 的向量​
    
    [justinsalamon.com](https://www.justinsalamon.com/uploads/4/3/9/4/4394963/kim_crepe_icassp_2018.pdf#:~:text=Figure%201,com%2Fmarl%2Fcrepe)
    
    。這360維的輸出對應預先劃分的360個音高值（覆蓋從約 C1 ~32.7 Hz 到 B7 ~1975.5 Hz 的範圍）​
    
    [stackoverflow.com](https://stackoverflow.com/questions/65898843/in-ml5-pitch-detection-using-crepe-model-how-to-detect-pitch-above-%C2%B12khz#:~:text=After%20more%20investigation%2C%20turns%20out,5%20Hz%20%28seen%20in%20paper)
    
    。模型使用 sigmoid 激活輸出，每個維度表示對應頻率的置信度，整個輸出向量大致形成在正確音高附近的高斯形狀​
    
    [justinsalamon.com](https://www.justinsalamon.com/uploads/4/3/9/4/4394963/kim_crepe_icassp_2018.pdf#:~:text=Estimation%20www,as%20in%20Equation%203%2C)
    
    ​
    
    [justinsalamon.com](https://www.justinsalamon.com/uploads/4/3/9/4/4394963/kim_crepe_icassp_2018.pdf#:~:text=Figure%201,com%2Fmarl%2Fcrepe)
    
    。最終預測的基頻透過對輸出向量加權平均或取最大值位置即可得到。總參數量方面，CREPE 的卷積層通道數逐層增加（例如每層128或256個濾波器等）​
    
    [mathworks.com](https://www.mathworks.com/help/audio/ref/crepe.html)
    
    ​
    
    [mathworks.com](https://www.mathworks.com/help/audio/ref/crepe.html)
    
    ，最終全連接層有 2048 × 360 個權重左右​
    
    [justinsalamon.com](https://www.justinsalamon.com/uploads/4/3/9/4/4394963/kim_crepe_icassp_2018.pdf#:~:text=Figure%201,com%2Fmarl%2Fcrepe)
    
    ，屬於一個中等規模的神經網路。
    
- **訓練數據與推論:** CREPE 是一個監督式學習模型，需要大量帶**真實音高標註**的音訊資料來訓練。作者使用了包含**人工合成音訊**和**真人錄音**的混合資料集：例如從 RWC 音樂資料庫合成6.1小時的單音音階音訊，以及若干真實獨唱旋律資料​
    
    [justinsalamon.com](https://www.justinsalamon.com/uploads/4/3/9/4/4394963/kim_crepe_icassp_2018.pdf#:~:text=1https%3A%2F%2Fgithub)
    
    。透過讓模型學習在各種音色、噪聲條件下對應正確的基頻，CREPE 獲得對不同來源訊號的強大泛化能力。在訓練過程中使用**分類架構**（每個頻率對應一個輸出節點）並採用交叉熵損失，但為了獲得連續的頻率預測，標註時將真值頻率附近的一段頻率區間標記為高斯形狀分布，以鼓勵模型對真值附近頻率都給出較高置信度​
    
    [justinsalamon.com](https://www.justinsalamon.com/uploads/4/3/9/4/4394963/kim_crepe_icassp_2018.pdf#:~:text=CREPE%20consists%20of%20a%20deep,pitch%20estimate%20is%20calculated%20deterministically)
    
    ​
    
    [justinsalamon.com](https://www.justinsalamon.com/uploads/4/3/9/4/4394963/kim_crepe_icassp_2018.pdf#:~:text=1https%3A%2F%2Fgithub)
    
    。這種技巧使得模型輸出對音高微小變化更平滑連續。推論時，音訊首先需要**分幀處理**（如每幀1024點，幀移256點），並**正規化**音量等級​
    
    [mathworks.com](https://www.mathworks.com/help/audio/ref/crepe.html#:~:text=The%20CREPE%20network%20requires%20you,postprocessing%20with%20pitch%20estimation%20using)
    
    。每一幀經過模型計算得到360維輸出向量，再轉換為一個實數頻率值。由於模型輸出採用密集分類方式，其頻率解析度約為每個音半音100分（100 cents）細分為5格，即20 cents左右的解析度**​
    
    [justinsalamon.com](https://www.justinsalamon.com/uploads/4/3/9/4/4394963/kim_crepe_icassp_2018.pdf#:~:text=Figure%201,pitch%20estimate%20is%20calculated%20deterministically)
    
    **。若需要更高或更低頻率範圍的預測，可相應地調整輸入幀長或重新訓練模型（例如 ml5.js 中使用的 CREPE 模型上限約 1975 Hz​
    
    [stackoverflow.com](https://stackoverflow.com/questions/65898843/in-ml5-pitch-detection-using-crepe-model-how-to-detect-pitch-above-%C2%B12khz#:~:text=After%20more%20investigation%2C%20turns%20out,5%20Hz%20%28seen%20in%20paper)
    
    ）。
    
- **計算需求:** 深度卷積網路的推理計算量通常比經典DSP演算法高。CREPE 每處理一幀音訊需要經過6層卷積和一次矩陣乘法，全過程涉及數十萬次乘加運算。然而，由於網路結構適中且卷積核尺寸較小，實際計算可以利用向量化和並行技術加速。在現代 CPU（尤其是支援SIMD向量指令的處理器）上，優化的 C/C++/Rust 實現可在數毫秒內完成單幀推理。例如，CREPE 的原作者提供的 Python/TF 實現配合 GPU 能達到即時處理，而在CPU上若使用TensorFlow優化程式庫也可勉強即時（每秒處理上百幀）​
    
    [justinsalamon.com](https://www.justinsalamon.com/uploads/4/3/9/4/4394963/kim_crepe_icassp_2018.pdf#:~:text=tracking%20based%20on%20a%20deep,CREPE%20are%20made%20available%20online1for)
    
    。不過，在資源受限的環境（如手機瀏覽器沒有硬體加速）下，執行 CREPE 可能會較吃力，需要權衡效能和功耗。計算需求也與分析的幀率和重疊程度有關——為平滑追蹤音高通常會重疊分析窗口，增加計算量。總體來說，CREPE 的計算需求**明顯高於傳統演算法**（可能高一到兩個數量級），但在高效實作下仍可達到接近即時的處理速度。
    
- **優缺點:** CREPE 的**優勢**在於極高的準確度和強大的泛化能力。研究顯示，CREPE 在多種測試資料上都優於傳統的演算法（如 pYIN、SWIPE），對噪聲也更具魯棒性​
    
    [justinsalamon.com](https://www.justinsalamon.com/uploads/4/3/9/4/4394963/kim_crepe_icassp_2018.pdf#:~:text=tracking%20based%20on%20a%20deep,CREPE%20are%20made%20available%20online1for)
    
    。即使是非典型音色（冷門樂器）或快速變化的音高曲線，CREPE 也能提供平滑且精確的追蹤​
    
    [justinsalamon.com](https://www.justinsalamon.com/uploads/4/3/9/4/4394963/kim_crepe_icassp_2018.pdf#:~:text=is%20not%20always%20the%20case%2C,a%20deep%20convolutional%20neural%20network)
    
    ​
    
    [justinsalamon.com](https://www.justinsalamon.com/uploads/4/3/9/4/4394963/kim_crepe_icassp_2018.pdf#:~:text=tracking%20based%20on%20a%20deep,CREPE%20are%20made%20available%20online1for)
    
    。此外，透過深度學習，免除了人工設計特徵和調參的過程，可直接從數據中學習到有效的週期性特徵表示。對於需要**絕對高精度**的應用（例如為旋律標註製作基準、或用於醫學聲音分析要求10音分以內誤差），CREPE 能達到>90%的準確率（10 cents 閾值）​
    
    [justinsalamon.com](https://www.justinsalamon.com/uploads/4/3/9/4/4394963/kim_crepe_icassp_2018.pdf#:~:text=CREPE%20,CREPE%20are%20made%20available%20online1for)
    
    。**缺點**是顯而易見的：模型體積較大（包含大量權重參數），需要消耗較多的記憶體與運算資源。相對於輕量的傳統方法，深度學習模型的推理延遲和能耗都更高。在沒有GPU加速的環境下，可能無法滿足嚴苛的即時要求或在低階裝置上運行。此外，CREPE 的結果雖然精確，但在應用中仍需要注意其頻率範圍限制（預訓練模型預期的頻率範圍之外可能無法準確預測，需要重新訓練模型擴展範圍）​
    
    [stackoverflow.com](https://stackoverflow.com/questions/65898843/in-ml5-pitch-detection-using-crepe-model-how-to-detect-pitch-above-%C2%B12khz#:~:text=After%20more%20investigation%2C%20turns%20out,5%20Hz%20%28seen%20in%20paper)
    
    。最後，開發和部署上也更為複雜——訓練需要大量帶標註的數據和算力支援，調整模型架構或遷移學習需要深度學習專業知識，這些都是傳統DSP方法所無需面對的負擔。
    
- **適用場景:** CREPE 非常適合那些**要求最高音高偵測準確率**且**訊號環境複雜**的場景。例如，在音樂訊息檢索領域，需要從現場錄音中抽取準確的旋律線；或者在研究中為多聲部旋律做準確標註、為歌聲合成建立高品質的基頻軌跡。亦或者在某些必須處理嘈雜背景的語音/動物聲音分析中，傳統方法頻繁失敗時，CREPE 可以提供更穩定的結果。值得注意的是，CREPE 雖然是單音追蹤器，但由於其強大的表示能力，甚至在一定程度上可應用於**複音樂音**的檢測（透過分析每時刻最強的基頻來近似處理，雖非真正多音辨識，但對主旋律提取有效）。另外，CREPE 模型已經預先訓練好，可直接應用於各種音源，而無需針對特定樂器調整，這對開發者來說相當便利。在實時應用如手機App或網頁應用中，如果能接受較高的CPU/GPU佔用，以獲取更精準的音高曲線，CREPE 也是值得考慮的方案。
    
- **Rust 與 WebAssembly 可行性:** 在 Rust/WASM 平台上使用 CREPE 面臨一些挑戰但並非不可行。由於 CREPE 涉及大量矩陣運算，最佳的做法是**利用現有的機器學習推理框架**。一種方式是在 Rust 中透過 FFI 調用如 TensorFlow Lite、ONNX Runtime 等C/C++推理引擎的 WASM 版本；另一種則是使用 TensorFlow.js 的 WASM 後端直接在瀏覽器中載入 CREPE 的模型權重。實際上，社群已經有將 CREPE 模型運用於瀏覽器的案例：例如 ml5.js 就提供了 CREPE 的即時音高偵測功能，底層採用 TensorFlow.js 並可選擇 WebAssembly 作為推理後端​
    
    [blog.tensorflow.org](https://blog.tensorflow.org/2020/03/introducing-webassembly-backend-for-tensorflow-js.html#:~:text=Introducing%20the%20WebAssembly%20backend%20for,alternative%20to%20the%20WebGL%20backend)
    
    。也有開發者將 CREPE 模型結合 WebAssembly 和 WebAudio 實現了即時的樂器調音應用​
    
    [github.com](https://github.com/alexcrist/autotone#:~:text=alexcrist%2Fautotone%3A%20A%20vocal%20pitch%20correction,API%3B%20Web%20workers%3B%20React)
    
    。如果希望以純 Rust 實現，可能需要借助像 `ndarray` 和 `matrixmultiply` 等數值庫編寫卷積層運算，或使用 Rust 的機器學習框架（如 `tch-rs` 基於 PyTorch，或 `burn`、`juice` 等）載入模型。但這些方案都相對複雜，涉及模型權重的序列化和在 WASM 中的載入。此外，模型權重檔案（數MB量級）需要隨應用下載，對網頁應用而言要考慮載入時間。性能方面，由於瀏覽器中的 WASM 尚無法直接利用GPU，只能靠CPU計算，深度卷積網路可能會佔用較高的CPU資源而影響頁面其他邏輯執行。因此在 Rust/WASM 中使用 CREPE 時，一般建議：**1)** 採用現成的 JS/WASM 深度學習庫以節省開發時間並獲得優化過的計算實現；**2)** 利用 Web Worker 將推理過程與主執行緒分離；**3)** 針對模型縮減，如使用 CREPE 提供的**精簡模型**（官方提供多種不同大小的模型權重）以降低計算量。如果應用對即時性要求極高（例如低於20ms延遲），可能需要重新評估是否使用這麼重的模型，或者在未來 WebAssembly 支援 SIMD/多執行緒完整成熟後再行嘗試。總而言之，CREPE 在 Rust 與 WebAssembly 上是**技術上可行**但實作成本和資源開銷較大的方案，在追求精度與性能平衡時需慎重評估。
    

## 4. **FFT 與頻譜分析**

- **頻譜分析的基本概念:** **頻譜分析**利用傅立葉變換將時間訊號轉換到頻率域，以觀察其各頻率成分的能量強度分布。對於週期訊號而言，其在頻域表現為一系列離散的譜峰（諧波）。基頻偵測可以透過在頻譜上尋找**基頻峰值**或分析諧波間距來實現。快速傅立葉變換（FFT）提供了一種高效計算頻譜的手段，使我們能在$O(N \log N)$的時間內得到 $N$ 點訊號的頻譜。頻譜分析的典型流程包括：對時間訊號乘上窗函數（減少頻譜洩漏），計算FFT得到複數頻譜，取其幅度譜（或功率譜），然後在感興趣的頻率範圍內搜尋峰值或模式。例如，最簡單的音高估計可直接取**最大幅度頻率分量**作為音高預測。然而實務中通常需要一些修正，如如果最高峰出現在高頻可能其實是某個泛音，還需結合次高峰等判斷基頻。
    
- **使用 FFT 進行音高偵測的方法:**
    
    - **峰值法：**直接尋找頻譜中的**最高峰**頻率作為基頻。如果需要精細結果，可對最高峰進行插值（例如採用鄰近三個頻點做拋物線內插）以突破FFT頻率解析度限制​
        
        [en.wikipedia.org](https://en.wikipedia.org/wiki/Pitch_detection_algorithm#:~:text=To%20improve%20on%20the%20pitch,9)
        
        。這種方法實作容易，但假設了基頻成分在頻譜中能量最大，對於泛音能量強於基頻的情況會失敗。
    - **諧波分析法：**利用樂音包含一系列諧波的特性，透過**辨識多個譜峰**並推斷其公因數頻率。例如**諧波積分類頻譜法**（Harmonic Product Spectrum, HPS）是一種經典技術，它將原始頻譜進行下採樣並與自身相乘，以增強基頻處的譜峰​
        
        [mdpi.com](https://www.mdpi.com/2076-3417/13/14/8191#:~:text=Time%20domain%20approaches%20are%20generally,The%20fundamental%20frequency)
        
        。具體做法是將頻譜拉伸（下採樣）為原來的 1/2、1/3 倍長度後逐點相乘，因為基頻的倍頻在下採樣後會對齊，累乘結果使基頻位置的能量被放大，從而更容易識別真正的基頻峰值。還有一些方法透過**譜峰間距**來判斷，例如計算臨近諧波間的距離是否相等，以此確定基頻。
    - **倒頻譜法：**這是一種將頻域訊號再次做傅立葉變換的方法。對數幅度譜做反向傅立葉可得到**倒頻譜（cepstrum）**，其中基頻會表現為倒頻譜中的一個峰值（倒頻譜的橫軸單位為倒頻率，對應時間軸即週期）。倒頻譜分析實際上等價於在頻域中做自相關，可將複雜頻譜轉換為尋找一個週期峰值的問題，適合在頻率成分眾多時濾除非周期性干擾。  
        總的來說，利用 FFT 進行音高偵測的方法多種多樣，從**簡單峰值法**到**諧波強化**、**倒頻譜**等都有應用。選擇何種技術取決於訊號性質和精度要求。
- **優缺點比較:** 頻域法的**優點**是計算速度快且在處理**複雜聲音**時可擴展性強。由於 FFT 高效，哪怕分析較長的窗口也能在短時間內完成​
    
    [en.wikipedia.org](https://en.wikipedia.org/wiki/Pitch_detection_algorithm#:~:text=Frequency%20domain%2C%20polyphonic%20detection%20is,suitably%20efficient%20for%20many%20purposes)
    
    。頻域方法還方便擴展到**多音高偵測**：例如透過尋找多個譜峰，可以一次性估計出和弦中幾個主要音高（雖然嚴格的多音分析需要更複雜演算法，但頻譜是良好的基礎）。另外，頻域分析自然適合融合**濾波**和**頻帶限制**等操作，可先隔離特定頻段再做偵測，提高抗干擾能力。透過譜線插值和相位分析等技術，頻域法亦能達到極高的頻率解析度，突破FFT本身的 bin 限制​
    
    [en.wikipedia.org](https://en.wikipedia.org/wiki/Pitch_detection_algorithm#:~:text=To%20improve%20on%20the%20pitch,9)
    
    。  
    **缺點**是頻域法對**基頻弱化**的情況較敏感——當基頻成分能量不占優時，簡單峰值法容易報錯八度（選到泛音頻率）​
    
    [en.wikipedia.org](https://en.wikipedia.org/wiki/Pitch_detection_algorithm#:~:text=difference%20function%20%29%2C%20ASMDF%20,citation%20needed)
    
    。雖然諧波分析可減輕此問題，但實作上也更複雜且需要假設輸入為諧波音。如果背景噪聲或非諧波成分較多，頻譜峰值會變得模糊。此外，頻域方法受制於**時間-頻率解析度**的折衷：為了精準定位低頻基音，需要較長的窗口（降低時間響應）；反之短窗口有利於追蹤快速變化，但低頻頻率解析度不足。另一個限制是**邊緣效應**和**譜洩漏**，需要適當的窗函數和零填充來減少誤差。相比時域方法，純頻域分析缺少對週期性的直接度量（如差分函數值），因此在穩定度上可能需要多幀平滑或結合動態規劃做軌跡平順化​
    
    [en.wikipedia.org](https://en.wikipedia.org/wiki/Pitch_detection_algorithm#:~:text=using%20an%20autocorrelation%20%20function,process%20in%20the%20other%20domain)
    
    。總體而言，頻域法適合於訊號譜特徵明顯或需要一次觀察整體頻率分布的情況，但在處理單一音高精準追蹤上，其可靠性有時不及經特殊設計的時域法（如 YIN）。
    
- **適用場景:** FFT 與頻譜分析廣泛應用於各類音高偵測場景。**樂器調音器**是其中經典的例子：將輸入聲音做FFT找出主頻峰值即可判斷音符高低，實現如吉他、鋼琴等樂器的調音。由於樂器聲音通常基頻在頻譜中佔有明顯峰值，且調音只需準確到半音甚至更粗略，簡單頻域法已足夠滿足需求。另一應用是在**多聲部音樂訊號**中作初步的頻率分析，例如和弦辨識中的候選基頻提取，可透過頻譜峰群組來推測可能的組成音高。頻域分析也常用於**聲學分析**中，例如鳥鳴或機械故障聲的頻率特徵提取。當我們關心**全局頻率內容**或需要同時觀察多個可能的基頻時，頻譜分析提供了豐富的信息。然而，在**即時音高追蹤**（如實時Pitch Tracking）方面，如果只針對單音訊號且需要極低延遲，實務上時域方法（YIN/自相關）可能更為穩定，因為它們對逐幀的細微變化不那麼敏感且不需要很大的頻率窗。此外，一些實時系統可能直接利用硬體FFT或現成的音訊處理API（如瀏覽器的ScriptProcessor或AudioWorkletNode結合FFT）來快速取得頻譜，再在主程式中分析，這在工程上也是頻域法的一個優勢：**易於使用現成工具**來加速開發。
    
- **Rust 及 WebAssembly 的實作方式:** 在 Rust 中可以方便地使用現有的 FFT 函式庫（例如 `rustfft` 或 `realfft`）計算訊號頻譜。這些庫性能優異，能利用Rust的安全並行特性，並且無需依賴 C 語言實作。將 Rust 編譯為 WebAssembly 後，在瀏覽器端即可執行 FFT 計算，速度接近原生。由於 FFT 運算本身已高度優化，其在 WASM 中的瓶頸主要在於資料傳輸和記憶體訪問。因此建議：**1)** 在 JavaScript 與 WASM 間傳遞音訊緩衝時使用高效方式，如 `Float32Array` 共享緩衝區，避免頻繁拷貝​
    
    [bojandjurdjevic.com](https://bojandjurdjevic.com/2018/WASM-vs-JS-Realtime-pitch-detection/#:~:text=let%20ptr%20%3D%20Module._malloc%28bufferLength%20,BYTES_PER_ELEMENT)
    
    ；**2)** 儘可能批量地做FFT計算，例如一次性對多幀做變換或使用重疊儲存避免重複搬移資料。若應用對性能要求極致，也可以考慮將**譜峰檢測的後處理**也放在 Rust 裡實現，減少WASM與JS之間的互動次數。事實上，瀏覽器本身亦提供一些頻譜分析的能力（如 Web Audio API 的 AnalyserNode），但其靈活性和精度有限且無法直接取得原始頻譜資料進一步計算​
    
    [bojandjurdjevic.com](https://bojandjurdjevic.com/2018/WASM-vs-JS-Realtime-pitch-detection/#:~:text=Note%20that%20this%20implementation%20is,seems%20like%20the%20best%20choice)
    
    。因此，在 Rust/WASM 中自行實作 FFT 音高偵測可得到最大的控制權。開發者也可以參考已有的開源實現：例如 **TarsosDSP** 是 Java 的音訊處理庫，其中包括頻域基頻偵測的實作；有人將其核心演算法移植到 Rust/WASM 環境並比較了效能，結果顯示 WASM 相較純 JS 提升顯著​
    
    [bojandjurdjevic.com](https://bojandjurdjevic.com/2018/WASM-vs-JS-Realtime-pitch-detection/#:~:text=To%20find%20it%20out%2C%20I,around)
    
    。最後，在 WASM 平臺上實作FFT時也可以打開 SIMD 支援（需確保目標瀏覽器版本支援 WASM SIMD），以進一步提升矩陣運算速度。總結而言，Rust/WASM 非常適合實作頻域音高偵測：憑藉 FFT 的效率和 Rust 的性能優勢，即使在瀏覽器中也能實現接近原生的即時頻譜分析和音高提取。
    

## **綜合比較與結論**

綜觀以上幾種音高偵測技術，各有其適用之處和權衡：

- **YIN** 算法基於自相關的創新改良，使其在**單音**音高追蹤中達到優異的**準確率與實時性平衡**。對於要求低延遲且精度高的一般應用（如實時旋律追蹤），YIN 是明智的選擇。Rust/WASM 能很好地承載此演算法，既保證速度又方便在瀏覽器部署。
    
- **傳統自相關法**概念雖簡單，但在精度和抗噪性上不如YIN。它適合作為**基礎方案**或教學用途，以及在**計算資源極度有限**的場景下勉強提供音高估計。在Rust/WASM中，若對精度要求不高，可以用較小代價實現自相關音高偵測，但若需進一步提高品質，通常會引入如YIN或其他改良。
    
- **CREPE** 代表了深度學習方法的高水準，在**困難情境**（噪聲、多種音色）下依然保持卓越的音高偵測效果。然而它對資源的需求使其更適合**離線分析**、**研究**或對性能要求沒那麼苛刻的應用。在瀏覽器等受限環境中使用時，需要借助機器學習庫和進行效能優化。當應用場景對精度要求到達無法透過傳統方法滿足時，才考慮引入CREPE，否則應權衡其帶來的複雜性。
    
- **FFT頻譜法**提供了一種**通用且高效**的框架，特別適合需要**頻譜全貌**或**多重音高**資訊的場合。對單音基頻偵測而言，頻域法在可靠性上可能略遜於YIN（特別是在基頻泛音能量分布特殊時），但勝在實作便利和運算快速。在Rust/WASM中利用FFT進行音高偵測是相當可行的方案，對於瀏覽器中的應用，若已經有音頻FFT的需求，不妨將音高偵測整合在頻域處理流程中完成。
    

最後，在 Rust 與 WebAssembly 平臺上實作音高偵測，**選擇適當的演算法**非常重要：需要在演算法複雜度、運算開銷、偵測精度之間取得平衡。對於一般的**即時音高偵測**（如調音、唱歌評分等），YIN 或優化的自相關法往往能滿足需求且實現相對簡單。而對於**專業分析**（如大型音頻資料庫離線處理，或要求極高精度的場合），可以考慮CREPE或其他深度學習模型。頻譜分析法則作為一種萬用工具，可靈活地與上述方法結合：例如先用FFT找出候選頻率，再以自相關法細化判斷。憑藉 Rust 的性能和 WebAssembly 的可移植性，我們完全可以在網頁前端實現過去需要本機應用才能完成的音高偵測功能，為各種創新的音樂和語音應用打下基礎。每種方法都有其最佳適用範圍，理解這些技術細節與差異，才能在實際開發中做出最適合的選擇。